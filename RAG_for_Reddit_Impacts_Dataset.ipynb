{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBBjZnMrhYFX"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/UKPLab/sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR9fyu_7hZpD"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before this:\n",
        "\n",
        "1. BIO to json.ipynb\n",
        "\n",
        "2. json to only certain label sentences.ipynb\n",
        "\n",
        "3. extract only sentence from tokens.ipynb"
      ],
      "metadata": {
        "id": "xfBltLj10va3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu3hWtGOPI6G"
      },
      "source": [
        "sentence bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Igmo_V7j5mfj"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Load the pretrained model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Step 1: Load and encode the test sentences\n",
        "with open('/content/only_sentences_impacts_test.txt', 'r') as test_file:\n",
        "    test_sentences = [line.strip() for line in test_file.readlines() if line.strip()]\n",
        "\n",
        "# Encode the test sentences\n",
        "test_embeddings = model.encode(test_sentences)\n",
        "\n",
        "# Set the value of k for top-k retrieval\n",
        "top_k = 5\n",
        "\n",
        "# Define the directory containing the train files (labels directory)\n",
        "labels_directory = '/content/labels'\n",
        "\n",
        "# Step 2: Pre-compute embeddings for all training files\n",
        "train_embeddings = {}\n",
        "train_sentences_dict = {}\n",
        "\n",
        "for filename in os.listdir(labels_directory):\n",
        "    file_path = os.path.join(labels_directory, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "        with open(file_path, 'r') as train_file:\n",
        "            training_sentences = [line.strip() for line in train_file.readlines() if line.strip()]\n",
        "\n",
        "        # Encode the training sentences in the current file\n",
        "        training_embeddings = model.encode(training_sentences)\n",
        "\n",
        "        # Store the embeddings and sentences in dictionaries for later use\n",
        "        train_embeddings[filename] = training_embeddings\n",
        "        train_sentences_dict[filename] = training_sentences\n",
        "\n",
        "# Open the output file in append mode to write each result step-by-step\n",
        "output_test_file_path = 'test_sentences_impacts_with_top_5_similar.txt'\n",
        "with open(output_test_file_path, 'w') as output_test_file:\n",
        "\n",
        "    # Step 3: For each test sentence, find the top-k most similar sentences from each train file\n",
        "    for idx, test_sentence in enumerate(test_sentences):\n",
        "        # Encode the current test sentence embedding\n",
        "        test_embedding = test_embeddings[idx].reshape(1, -1)\n",
        "\n",
        "        # Store top-k similar sentences from all files for the current test sentence\n",
        "        all_retrieved_sentences = []\n",
        "\n",
        "        # Step 4: Iterate through each precomputed training file embeddings\n",
        "        for filename, training_embeddings in train_embeddings.items():\n",
        "            training_sentences = train_sentences_dict[filename]\n",
        "\n",
        "            # Step 5: Calculate cosine similarity between the test sentence and all training sentences in the current file\n",
        "            similarities = cosine_similarity(test_embedding, training_embeddings)\n",
        "\n",
        "            # Step 6: Get the top-K most similar sentences\n",
        "            top_k_indices = np.argsort(similarities[0])[::-1][:top_k]\n",
        "\n",
        "            # Fetch the top-k most similar sentences with their source file information\n",
        "            retrieved_sentences = [f\"{training_sentences[i]} (from {filename})\" for i in top_k_indices]\n",
        "\n",
        "            # Add the top-k sentences from the current file to the list for this test sentence\n",
        "            all_retrieved_sentences.extend(retrieved_sentences)\n",
        "\n",
        "        # Step 7: Combine all retrieved sentences into the desired output format\n",
        "        formatted_output = f\"Input Sentence: {test_sentence}\\nTop-{top_k} Similar Sentences from labels:\\n\" + \"\\n\".join(all_retrieved_sentences) + \"\\n\\n\"\n",
        "\n",
        "        # Write the formatted output for the current test sentence to the output file\n",
        "        output_test_file.write(formatted_output)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Processed test sentence {idx + 1}/{len(test_sentences)}\")\n",
        "\n",
        "print(f\"Results saved to {output_test_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSxvT6RbPKda"
      },
      "source": [
        "tf-idf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UbozSyAooDdJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Step 1: Load the test sentences\n",
        "with open('/content/only_sentences_impacts_test.txt', 'r') as test_file:\n",
        "    test_sentences = [line.strip() for line in test_file.readlines() if line.strip()]\n",
        "\n",
        "# Set the value of k for top-k retrieval\n",
        "top_k = 10\n",
        "\n",
        "# Define the directory containing the train files (labels directory)\n",
        "labels_directory = '/content/labels'\n",
        "\n",
        "# Step 2: Load all training sentences from all files\n",
        "all_training_sentences = []\n",
        "train_sentences_dict = {}\n",
        "\n",
        "for filename in os.listdir(labels_directory):\n",
        "    file_path = os.path.join(labels_directory, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "        with open(file_path, 'r') as train_file:\n",
        "            training_sentences = [line.strip() for line in train_file.readlines() if line.strip()]\n",
        "            all_training_sentences.extend(training_sentences)\n",
        "            train_sentences_dict[filename] = training_sentences\n",
        "\n",
        "# Step 3: Initialize the TF-IDF vectorizer and fit it on all training sentences\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(all_training_sentences)\n",
        "\n",
        "# Step 4: Transform the training sentences from each file and store their embeddings\n",
        "train_embeddings = {}\n",
        "\n",
        "for filename, training_sentences in train_sentences_dict.items():\n",
        "    training_embeddings = vectorizer.transform(training_sentences)\n",
        "    train_embeddings[filename] = training_embeddings\n",
        "\n",
        "# Step 5: Transform the test sentences using the same vectorizer\n",
        "test_embeddings = vectorizer.transform(test_sentences)\n",
        "\n",
        "# Open the output file in append mode to write each result step-by-step\n",
        "output_test_file_path = 'Impacts_test_sentences_with_top_10_similar_tf_idf.txt'\n",
        "with open(output_test_file_path, 'w') as output_test_file:\n",
        "\n",
        "    # Step 6: For each test sentence, find the top-k most similar sentences from each train file\n",
        "    for idx, test_sentence in enumerate(test_sentences):\n",
        "        # Get the TF-IDF vector for the current test sentence\n",
        "        test_embedding = test_embeddings[idx].reshape(1, -1)\n",
        "\n",
        "        # Store top-k similar sentences from all files for the current test sentence\n",
        "        all_retrieved_sentences = []\n",
        "\n",
        "        # Step 7: Iterate through each precomputed training file embeddings\n",
        "        for filename, training_embeddings in train_embeddings.items():\n",
        "            training_sentences = train_sentences_dict[filename]\n",
        "\n",
        "            # Step 8: Calculate cosine similarity between the test sentence and all training sentences in the current file\n",
        "            similarities = cosine_similarity(test_embedding, training_embeddings)\n",
        "\n",
        "            # Step 9: Get the top-K most similar sentences\n",
        "            top_k_indices = np.argsort(similarities[0])[::-1][:top_k]\n",
        "\n",
        "            # Fetch the top-k most similar sentences with their source file information\n",
        "            retrieved_sentences = [f\"{training_sentences[i]} (from {filename})\" for i in top_k_indices]\n",
        "\n",
        "            # Add the top-k sentences from the current file to the list for this test sentence\n",
        "            all_retrieved_sentences.extend(retrieved_sentences)\n",
        "\n",
        "        # Step 10: Combine all retrieved sentences into the desired output format\n",
        "        formatted_output = f\"Input Sentence: {test_sentence}\\nTop-{top_k} Similar Sentences from labels:\\n\" + \"\\n\".join(all_retrieved_sentences) + \"\\n\\n\"\n",
        "\n",
        "        # Write the formatted output for the current test sentence to the output file\n",
        "        output_test_file.write(formatted_output)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Processed test sentence {idx + 1}/{len(test_sentences)}\")\n",
        "\n",
        "print(f\"Results saved to {output_test_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSA (Latent Semantic Analysis)\n"
      ],
      "metadata": {
        "id": "vak_EZvh0XTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load and preprocess the training sentences\n",
        "with open('/content/filtered_sentences_only_impacts_traind_dev.txt', 'r') as file:\n",
        "    training_sentences = [line.strip() for line in file.readlines() if line.strip()]\n",
        "\n",
        "# Step 2: Load the test sentences from the provided file\n",
        "with open('/content/only_sentences_impacts_test.txt', 'r') as test_file:\n",
        "    test_sentences = [line.strip() for line in test_file.readlines() if line.strip()]\n",
        "\n",
        "# Step 3: Initialize the TF-IDF vectorizer and fit it to the training sentences\n",
        "vectorizer = TfidfVectorizer()\n",
        "training_tfidf = vectorizer.fit_transform(training_sentences)\n",
        "\n",
        "# Step 4: Apply Latent Semantic Analysis (LSA) using Truncated SVD on the TF-IDF matrix\n",
        "n_components = 100  # Number of latent semantic dimensions (this can be tuned)\n",
        "svd = TruncatedSVD(n_components=n_components)\n",
        "training_lsa = svd.fit_transform(training_tfidf)\n",
        "\n",
        "# Step 5: Transform the test sentences using the same vectorizer and SVD model\n",
        "test_tfidf = vectorizer.transform(test_sentences)\n",
        "test_lsa = svd.transform(test_tfidf)\n",
        "\n",
        "# Set the value of k for top-k retrieval\n",
        "top_k = 10\n",
        "\n",
        "# Store results for each test sentence\n",
        "results = []\n",
        "\n",
        "# Step 6: For each test sentence, calculate cosine similarity with the training sentences\n",
        "for i, input_sentence in enumerate(test_sentences):\n",
        "    # Compute cosine similarity between the current test sentence and all training sentences\n",
        "    similarities = cosine_similarity(test_lsa[i].reshape(1, -1), training_lsa)\n",
        "\n",
        "    # Get the indices of the top-K most similar training sentences\n",
        "    top_k_indices = np.argsort(similarities[0])[::-1][:top_k]\n",
        "\n",
        "    # Fetch the top-k most similar sentences from the training set\n",
        "    retrieved_sentences = [training_sentences[idx] for idx in top_k_indices]\n",
        "\n",
        "    # Store the input sentence and its retrieved top-k similar sentences\n",
        "    results.append(f\"Input Sentence: {input_sentence}\\nTop-{top_k} Similar Sentences:\\n\" + \"\\n\".join(retrieved_sentences) + \"\\n\\n\")\n",
        "\n",
        "# Step 7: Write the results to a new file\n",
        "output_test_file_path = 'test_sentences_with_top_10_similar_LSA.txt'\n",
        "with open(output_test_file_path, 'w') as output_test_file:\n",
        "    output_test_file.writelines(results)\n",
        "\n",
        "print(f\"Results saved to {output_test_file_path}\")\n"
      ],
      "metadata": {
        "id": "UfOCtbkh0aEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dense Passage Retrieval (DPR)"
      ],
      "metadata": {
        "id": "ic0ooZmI0hZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the pretrained DPR model and tokenizer for context and question encoding\n",
        "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "context_model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "question_model = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "\n",
        "# Step 2: Load and preprocess the training sentences\n",
        "with open('/content/filtered_sentences_only_impacts_traind_dev.txt', 'r') as file:\n",
        "    training_sentences = [line.strip() for line in file.readlines() if line.strip()]\n",
        "\n",
        "# Step 3: Encode the training sentences using the DPR context encoder\n",
        "training_embeddings = []\n",
        "for sentence in training_sentences:\n",
        "    inputs = context_tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    embeddings = context_model(**inputs).pooler_output\n",
        "    training_embeddings.append(embeddings.detach().numpy())\n",
        "\n",
        "# Convert list to numpy array for easier handling\n",
        "training_embeddings = np.vstack(training_embeddings)\n",
        "\n",
        "# Step 4: Load the test sentences from the provided file\n",
        "with open('/content/only_sentences_impacts_test.txt', 'r') as test_file:\n",
        "    test_sentences = [line.strip() for line in test_file.readlines() if line.strip()]\n",
        "\n",
        "# Set the value of k for top-k retrieval\n",
        "top_k = 10\n",
        "\n",
        "# Store results for each test sentence\n",
        "results = []\n",
        "\n",
        "# Step 5: For each test sentence, encode it using the DPR question encoder\n",
        "for input_sentence in test_sentences:\n",
        "    inputs = question_tokenizer(input_sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    input_embedding = question_model(**inputs).pooler_output.detach().numpy()\n",
        "\n",
        "    # Step 6: Calculate cosine similarity between the input sentence and all training sentences\n",
        "    similarities = cosine_similarity(input_embedding, training_embeddings)\n",
        "\n",
        "    # Step 7: Get the top-K most similar sentences\n",
        "    top_k_indices = np.argsort(similarities[0])[::-1][:top_k]\n",
        "\n",
        "    # Fetch the top-k most similar sentences from the training set\n",
        "    retrieved_sentences = [training_sentences[idx] for idx in top_k_indices]\n",
        "\n",
        "    # Store the input sentence and its retrieved top-k similar sentences\n",
        "    results.append(f\"Input Sentence: {input_sentence}\\nTop-{top_k} Similar Sentences:\\n\" + \"\\n\".join(retrieved_sentences) + \"\\n\\n\")\n",
        "\n",
        "# Step 8: Write the results to a new file\n",
        "output_test_file_path = 'Impacts_test_sentences_with_top_10_similar_dpr.txt'\n",
        "with open(output_test_file_path, 'w') as output_test_file:\n",
        "    output_test_file.writelines(results)\n",
        "\n",
        "print(f\"Results saved to {output_test_file_path}\")\n"
      ],
      "metadata": {
        "id": "FT49o8-m0j2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOV4n7JH2Wrq"
      },
      "source": [
        "\n",
        "\n",
        "step:\n",
        "\n",
        "  1. for each test sentence, find Top-k similar sentences\n",
        "  \n",
        "  2. for each Top-k similar sentences, change to tokens and labels format\n",
        "  \n",
        "  3. put it into prompt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj5F5DDbfatb"
      },
      "outputs": [],
      "source": [
        "## check if any of the similar sentences are smaller than k\n",
        "def check_sentences(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    current_input_sentence = None\n",
        "    similar_sentences_count = 0\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        if line.startswith(\"Input Sentence:\"):\n",
        "            current_input_sentence = line.replace(\"Input Sentence:\", \"\").strip()\n",
        "            similar_sentences_count = 0\n",
        "\n",
        "        elif line.startswith(\"Top-10 Similar Sentences:\"):\n",
        "            similar_sentences_count = 0\n",
        "\n",
        "        elif line:\n",
        "            similar_sentences_count += 1\n",
        "\n",
        "        if similar_sentences_count > 0 and similar_sentences_count < 10 and (not line or line == \"Input Sentence:\"):\n",
        "            print(f\"Input sentence with less than 20 similar sentences: {current_input_sentence}\")\n",
        "            similar_sentences_count = 0  # Reset for the next input sentence\n",
        "\n",
        "# Path to your file\n",
        "file_path = '/content/impacts_test_sentences_with_top_10_similar_colbert.txt'\n",
        "\n",
        "# Call the function to check and print sentences\n",
        "check_sentences(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXlkGTeFzJjE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "\n",
        "# Load the JSON data\n",
        "json_file_path = '/content/train_dev'\n",
        "test_file_path = '/content/test_sentences_with_top_10_similar_dpr.txt'\n",
        "output_file_path = 'Impacts_output_with_matches_top_10_similar_dpr.txt'\n",
        "\n",
        "# Load JSON data with tokens and labels\n",
        "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "    json_data = [json.loads(line) for line in f]\n",
        "\n",
        "# Load pre-trained SBERT model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Create embeddings for JSON data\n",
        "json_texts = [\" \".join(item[\"text\"]).lower() for item in json_data]\n",
        "json_embeddings = model.encode(json_texts, convert_to_tensor=True)\n",
        "\n",
        "# Cache dictionary to store best match information\n",
        "best_match_cache = {}\n",
        "\n",
        "# Function to find the best match based on similarity threshold\n",
        "def find_best_match(similar_sentence, json_data, json_embeddings, model, threshold=0.5):\n",
        "    # Check if similar_sentence already exists in the cache\n",
        "    if similar_sentence in best_match_cache:\n",
        "        return best_match_cache[similar_sentence]\n",
        "\n",
        "    similar_embedding = model.encode(similar_sentence.lower(), convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarity with all json_embeddings\n",
        "    similarities = util.pytorch_cos_sim(similar_embedding, json_embeddings)[0]\n",
        "\n",
        "    # Get the indices sorted by similarity in descending order\n",
        "    sorted_indices = similarities.argsort(descending=True)\n",
        "\n",
        "    # Iterate through sorted indices to find the first match above threshold\n",
        "    for index in sorted_indices:\n",
        "        if similarities[index] >= threshold:\n",
        "            best_match = json_texts[index]\n",
        "            best_tokens = json_data[index][\"text\"]\n",
        "            best_labels = json_data[index][\"label\"]\n",
        "            # Cache the result\n",
        "            best_match_cache[similar_sentence] = (best_tokens, best_labels, best_match, similarities[index].item())\n",
        "            return best_tokens, best_labels, best_match, similarities[index].item()\n",
        "\n",
        "    # If no match above threshold, return None and cache the result\n",
        "    best_match_cache[similar_sentence] = (None, None, None, None)\n",
        "    return None, None, None, None\n",
        "\n",
        "# Process the test sentences and similar sentences\n",
        "with open(test_file_path, 'r', encoding='utf-8') as test_file, open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    lines = test_file.readlines()\n",
        "\n",
        "    for i in range(0, len(lines), 13):\n",
        "        original_sentence = lines[i].strip().replace(\"Input Sentence: \", \"\")\n",
        "        #print(original_sentence)\n",
        "        similar_sentences = [lines[i+j].strip() for j in range(2, 12)]\n",
        "\n",
        "        output_file.write(f\"Original Sentence: {original_sentence}\\n\")\n",
        "\n",
        "        # Process each similar sentence and find the first best match\n",
        "        for similar_sentence in similar_sentences:\n",
        "            # Find best match for the similar sentence\n",
        "            tokens, labels, match, similarity = find_best_match(similar_sentence, json_data, json_embeddings, model)\n",
        "            if tokens and labels:\n",
        "                output_file.write(f\"Best Match: {match} (Similarity: {similarity:.2f})\\n\")\n",
        "                output_file.write(f\"Tokens: {tokens}\\n\")\n",
        "                output_file.write(f\"Labels: {labels}\\n\\n\")\n",
        "            else:\n",
        "                output_file.write(f\"No match found for: {similar_sentence}\\n\\n\")\n",
        "\n",
        "        output_file.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Output saved to {output_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Fl7L0Yc3zJ0h"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# File paths\n",
        "test_texts4_file = '/content/test_texts.txt'\n",
        "output_file_path = '/content/Impacts_output_with_matches_top_10_similar_dpr.txt'\n",
        "final_output_path = '/content/Impacts_output_with_matches_top_10_similar_dpr.txt'\n",
        "\n",
        "# Load tokens from test_texts4 copy.txt\n",
        "with open(test_texts4_file, 'r', encoding='utf-8') as f:\n",
        "    tokens_data = [line.strip() for line in f.readlines()]  # Read each line as a string of tokens\n",
        "\n",
        "# Load original sentences from the output file\n",
        "with open(output_file_path, 'r', encoding='utf-8') as f:\n",
        "    output_lines = f.readlines()\n",
        "\n",
        "# Replace original sentences with tokens and write to the new file\n",
        "token_idx = 0\n",
        "with open(final_output_path, 'w', encoding='utf-8') as final_output:\n",
        "    for line in output_lines:\n",
        "        if line.startswith(\"Original Sentence:\"):\n",
        "            # Replace original sentence witah corresponding tokens from tokens_data\n",
        "            #print(token_idx)\n",
        "            #print(f\"Original Sentence: {tokens_data[token_idx]}\")\n",
        "            final_output.write(f\"Original Sentence: {tokens_data[token_idx]}\\n\\n\")\n",
        "            token_idx += 1  # Move to the next set of tokens\n",
        "        else:\n",
        "            # Keep the rest of the lines unchanged\n",
        "            final_output.write(line)\n",
        "\n",
        "print(f\"Processed file saved to {final_output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_CHKG1i-16X"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Format the Best Match text\n",
        "def format_best_match_text(data):\n",
        "    tokens = data['Tokens']\n",
        "    labels = data['Labels']\n",
        "\n",
        "    # Reformat tokens with their corresponding labels\n",
        "    formatted_list = []\n",
        "    for i, word in enumerate(tokens):\n",
        "        label = labels[i].replace('_', ' ').strip(\"'\\\"\")\n",
        "        if labels[i] != 'O':\n",
        "            formatted_list.append(f\"'{word}-{label}'\")\n",
        "        else:\n",
        "            formatted_list.append(f\"'{word}-O'\")\n",
        "\n",
        "    return formatted_list\n",
        "\n",
        "# Extract the Best Match section from the file\n",
        "def extract_best_match_from_file(file_content):\n",
        "    matches = []\n",
        "    match_blocks = re.findall(r\"(Best Match:[\\s\\S]+?Labels: \\[(.*?)\\])\", file_content)\n",
        "\n",
        "    for block, label_string in match_blocks:\n",
        "        tokens_match = re.findall(r\"Tokens: \\[(.*?)\\]\", block)\n",
        "\n",
        "        if tokens_match:\n",
        "            tokens = tokens_match[0].replace(\"'\", \"\").split(\", \")\n",
        "            labels = label_string.split(\", \")\n",
        "            matches.append({'Tokens': tokens, 'Labels': labels})\n",
        "        else:\n",
        "            print(\"Tokens not found, skipping this block\")\n",
        "\n",
        "    return matches\n",
        "\n",
        "# Step-by-step replace Labels in the file and write to the output file\n",
        "def replace_and_write_file_content(input_file_path, output_file_path):\n",
        "    # Read the content of the input file\n",
        "    with open(input_file_path, 'r') as f:\n",
        "        file_content = f.read()\n",
        "\n",
        "    # Extract tokens and labels from the Best Match section\n",
        "    best_matches = extract_best_match_from_file(file_content)\n",
        "\n",
        "    # Open the output file for step-by-step writing\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        current_pos = 0  # Current processing position\n",
        "\n",
        "        # Iterate through all extracted Best Match blocks\n",
        "        for idx, match in enumerate(best_matches):\n",
        "            try:\n",
        "                original_labels = \", \".join(match['Labels'])\n",
        "                formatted_labels = \", \".join(format_best_match_text(match))\n",
        "\n",
        "                # Locate the position of the current original_labels in the file content\n",
        "                pos = file_content.find(original_labels, current_pos)\n",
        "                if pos == -1:\n",
        "                    print(f\"Original labels not found: {original_labels}\")\n",
        "                    continue\n",
        "\n",
        "                # Write the portion of the current file content up to the replacement point into the output file\n",
        "                output_file.write(file_content[current_pos:pos])\n",
        "                # write new labels\n",
        "                output_file.write(formatted_labels)\n",
        "\n",
        "                # Update current_pos to the next processing position\n",
        "                current_pos = pos + len(original_labels)\n",
        "\n",
        "                # Print progress information\n",
        "                print(f\"Processed {idx + 1}/{len(best_matches)} 个 Best Match\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred while processing {idx + 1} Best Match: {str(e)}\")\n",
        "\n",
        "        # Write the remaining unprocessed file content\n",
        "        output_file.write(file_content[current_pos:])\n",
        "\n",
        "    print(f\"Updated file has been written {output_file_path}\")\n",
        "\n",
        "# file path\n",
        "input_file_path = '/content/Impacts_output_with_matches_top_10_similar_dpr.txt'\n",
        "output_file_path = '/content/Impacts_tokens_with_updated_labels_top_10_dpr.txt'\n",
        "\n",
        "# Call the function to process and write step by step\n",
        "replace_and_write_file_content(input_file_path, output_file_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}