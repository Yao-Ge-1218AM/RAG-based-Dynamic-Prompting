{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyFymKvuiVv5"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/stanford-futuredata/ColBERT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ColBERT"
      ],
      "metadata": {
        "id": "w5LLDzqPiaPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ujson"
      ],
      "metadata": {
        "id": "BNFNjRfxicyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colbert"
      ],
      "metadata": {
        "id": "kSs7p2OaiejD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colbert-ai[torch,faiss-gpu]"
      ],
      "metadata": {
        "id": "d5FK5YL5if92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/colbertv2.0"
      ],
      "metadata": {
        "id": "mLfzTs1BzVbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf /content/colbertv2.0.tar.gz -C /content/"
      ],
      "metadata": {
        "id": "aTxXE3yGihco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the training dataset to ColBERT collection format\n",
        "with open('/content/filtered_sentences_only_impacts_traind_dev.txt', 'r') as file:\n",
        "    training_sentences = [line.strip() for line in file.readlines() if line.strip()]\n",
        "\n",
        "# Write the training data in collection format, with the format: id\\tSentence\n",
        "collection_path = \"/content/train_impacts_collection.tsv\"\n",
        "with open(collection_path, 'w') as f:\n",
        "    for idx, sentence in enumerate(training_sentences):\n",
        "        f.write(f'{idx}\\t{sentence}\\n')\n"
      ],
      "metadata": {
        "id": "ar8cJJMaikH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the test dataset to ColBERT collection format\n",
        "with open('/content/only_sentences_impacts_test.txt', 'r') as file:\n",
        "    test_sentences = [line.strip() for line in file.readlines() if line.strip()]\n",
        "\n",
        "# Write the test data in collection format, with the format: id\\tSentence\n",
        "query_path = \"/content/queries.tsv\"\n",
        "with open(query_path, 'w') as f:\n",
        "    for idx, sentence in enumerate(test_sentences):\n",
        "        f.write(f'{idx}\\t{sentence}\\n')"
      ],
      "metadata": {
        "id": "vnFUzmXKio3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "# Set logging format and output level\n",
        "#logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
        "\n",
        "# Enable more detailed logging\n",
        "#logger = logging.getLogger()\n",
        "#logger.setLevel(logging.DEBUG)\n",
        "\n",
        "\n",
        "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "from colbert import Indexer\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Step 1: Create the ColBERT runtime environment\n",
        "    with Run().context(RunConfig(nranks=1, experiment=\"my_experiment\", avoid_fork_if_possible=True)):\n",
        "\n",
        "        # Step 2: Configure ColBERT settings\n",
        "        config = ColBERTConfig(\n",
        "            nbits=2,  # Compression bits set to 2\n",
        "            root=\"/content/ColBERT/experiments\",  # Root directory for experiment output files\n",
        "        )\n",
        "\n",
        "        # Step 3: Create an Indexer object and load the model checkpoint\n",
        "        indexer = Indexer(\n",
        "            checkpoint=\"/content/colbertv2.0/\",  # Path to the model checkpoint\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "        # Step 4: Build the index and specify the collection.tsv file path\n",
        "        indexer.index(\n",
        "            name=\"my_train_impacts_collection.nbits=2\",  # Name of the index\n",
        "            collection=\"/content/train_impacts_collection.tsv\"  # Path to the collection file\n",
        "        )\n"
      ],
      "metadata": {
        "id": "crcvOUxtirHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from colbert.data import Queries\n",
        "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "from colbert import Searcher\n",
        "import os\n",
        "\n",
        "# Step 1: Configure the runtime environment and ColBERT settings\n",
        "with Run().context(RunConfig(nranks=1, experiment=\"my_experiment\")):\n",
        "\n",
        "    config = ColBERTConfig(\n",
        "        root=\"/content/ColBERT/experiments\",  # Path for experiment results\n",
        "    )\n",
        "\n",
        "    # Step 2: Load test sentences into Queries\n",
        "    queries = Queries('/content/queries.tsv')\n",
        "\n",
        "    top_k = 10  # Number of sentences to retrieve from each collection file\n",
        "\n",
        "    # Define the directory containing training set files\n",
        "    labels_directory = '/content/labels_collections'\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Step 3: Perform retrieval for each query sentence\n",
        "    for qid, query_text in queries.items():\n",
        "        all_retrieved_sentences = []  # To store similar sentences from multiple collection files\n",
        "\n",
        "        # Iterate through each collection file, load the corresponding index, and perform retrieval\n",
        "        for filename in os.listdir(labels_directory):\n",
        "            file_path = os.path.join(labels_directory, filename)\n",
        "\n",
        "            # If the current file is a collection file, perform retrieval\n",
        "            if os.path.isfile(file_path) and filename.endswith('.tsv'):\n",
        "                # Extract the index name, e.g., my_label1.nbits=2\n",
        "                index_name = f\"my_{filename.split('.')[0]}.nbits=2\"\n",
        "\n",
        "                # Initialize the Searcher and load the generated index\n",
        "                searcher = Searcher(index=f\"/content/ColBERT/experiments/my_experiment/indexes/{index_name}\", config=config)\n",
        "\n",
        "                # Perform retrieval for the current query sentence\n",
        "                pids, ranks, scores = searcher.search(query_text, k=top_k)\n",
        "\n",
        "                # Retrieve document IDs and content, and save the results\n",
        "                retrieved_sentences = [f\"{searcher.collection[pid]} (from {filename})\" for pid in pids[:top_k]]  # Only fetch the top_k similar sentences\n",
        "\n",
        "                # Add the similar sentences from the current collection to the overall results\n",
        "                all_retrieved_sentences.extend(retrieved_sentences)\n",
        "\n",
        "        # Save the input sentence and all retrieval results\n",
        "        results.append(f\"Input Sentence: {query_text}\\nTop-{top_k * len(os.listdir(labels_directory))} Similar Sentences:\\n\" + \"\\n\".join(all_retrieved_sentences) + \"\\n\\n\")\n",
        "\n",
        "        print(f\"Processed test sentence {qid + 1}/{len(queries.items())}\")\n",
        "\n",
        "    # Step 4: Save the results to a file\n",
        "    output_test_file_path = '/content/Impacts_test_sentences_with_top_10_similar_colbert.txt'\n",
        "    with open(output_test_file_path, 'w') as output_test_file:\n",
        "        output_test_file.writelines(results)\n",
        "\n",
        "    print(f\"Results saved to {output_test_file_path}\")\n"
      ],
      "metadata": {
        "id": "CgLq1uLjgnGI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}