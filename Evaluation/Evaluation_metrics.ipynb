{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We can provide both relaxed and strict F1 score.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gfc0U3nv4yRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "strict F1 score can be calculated directly using seqeval."
      ],
      "metadata": {
        "id": "5i-I7p015QU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "4Y1tTnr04jH0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install seqeval\n",
        "from seqeval.metrics import classification_report\n",
        "from seqeval.metrics import f1_score\n",
        "\n",
        "# Example ground truth (true labels) and predictions\n",
        "true_labels = [['O', 'I-PER', 'I-PER', 'O'], ['I-LOC'], ['I-LOC']]\n",
        "predicted_labels = [['O', 'I-PER', 'I-PER', 'O'], ['I-LOC'], ['O']]\n",
        "\n",
        "#predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
        "#references = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(true_labels, predicted_labels)\n",
        "f1_score = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "print(\"F1 Score:\", f1_score)\n",
        "\n",
        "#results = seqeval.compute(predictions=predictions, references=references)\n",
        "# Print the F1-score\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "dD4GHU-E436i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "consier token-level"
      ],
      "metadata": {
        "id": "4bvPFAXXH59b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_token_level_f1(true_labels, predicted_labels):\n",
        "    total_tokens = len(true_labels)\n",
        "    tp, fp, fn = 0, 0, 0\n",
        "\n",
        "    for true_label, pred_label in zip(true_labels, predicted_labels):\n",
        "        for true_token, pred_token in zip(true_label, pred_label):\n",
        "            if true_token == pred_token and true_token != 'O':\n",
        "                tp += 1\n",
        "            elif true_token == 'O' and pred_token != 'O':\n",
        "                fp += 1\n",
        "            elif true_token != 'O' and pred_token == 'O':\n",
        "                fn += 1\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return f1_score\n",
        "\n",
        "# Example usage:\n",
        "#true_labels = [['B-PER', 'I-PER', 'O', 'B-LOC', 'O']]\n",
        "#predicted_labels = [['B-PER', 'O', 'O', 'B-LOC', 'O']]\n",
        "\n",
        "#true_labels = [['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'I-LOC']]\n",
        "#predicted_labels = [['O', 'I-PER', 'I-PER', 'O', 'B-LOC', 'O']]\n",
        "\n",
        "token_level_f1 = calculate_token_level_f1(gold_labels, pred_labels)\n",
        "print(\"Token-level F1 Score:\", token_level_f1)\n"
      ],
      "metadata": {
        "id": "emDSMZH35WR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For relaxed F1 score:"
      ],
      "metadata": {
        "id": "z8Aov23n5UKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "International Workshop on Semantic Evaluation (SemEval)\n",
        "\n",
        "The SemEvalâ€™13 introduced four different ways to measure precision/recall/f1-score results based on the metrics defined by MUC (Message Understanding Conference).\n",
        "\n",
        "    Strict: exact boundary surface string match and entity type;\n",
        "\n",
        "    Exact: exact boundary match over the surface string, regardless of the type;\n",
        "\n",
        "    Partial: partial boundary match over the surface string, regardless of the type;\n",
        "\n",
        "    Type: some overlap between the system tagged entity and the gold annotation is required;\n",
        "\n",
        "each of these ways to measure the performance accounts for correct, incorrect, partial, missed and spurious in different ways.\n",
        "\n",
        "Note you can find the complete code for this evaluation metrics on this repository:\n",
        "\n",
        "    https://github.com/davidsbatista/NER-Evaluation\n",
        "\n",
        "You can find a more detailed explanation in the following blog post:\n",
        "\n",
        "    http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\n",
        "\n"
      ],
      "metadata": {
        "id": "cH9xqINiy0VA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/davidsbatista/NER-Evaluation"
      ],
      "metadata": {
        "id": "JSRZLyvZTorX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "change folder name \"NER-Evaluation\" to \"NER_Evaluation\""
      ],
      "metadata": {
        "id": "jfk67AkNyByD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "\n",
        "import NER_Evaluation.ner_evaluation.ner_eval\n",
        "\n",
        "from NER_Evaluation.ner_evaluation.ner_eval import collect_named_entities\n",
        "from NER_Evaluation.ner_evaluation.ner_eval import compute_metrics\n",
        "from NER_Evaluation.ner_evaluation.ner_eval import compute_precision_recall_wrapper"
      ],
      "metadata": {
        "id": "cdFzdG3iWEoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_new = '/content/impacts_golden_labels.txt'\n",
        "\n",
        "gold_labels = []  # Initialize an empty list for the predicted labels\n",
        "\n",
        "with open(file_path_new, 'r', encoding='utf-8') as file:  # Ensure proper encoding\n",
        "    for line in file:\n",
        "        line_labels = line.strip().split()  # Split each line into labels\n",
        "        gold_labels.append(line_labels)  # Append the list of labels to the main list\n",
        "\n",
        "# Show the first 5 entries of pred_labels to verify the operation\n",
        "gold_labels[:5]"
      ],
      "metadata": {
        "id": "DZEMK9qobt9A",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-attempt processing the file to create a nested list structure for pred_labels\n",
        "file_path_new = '/content/predictions-GPT4-impacts-dpr.txt'\n",
        "\n",
        "pred_labels = []  # Initialize an empty list for the predicted labels\n",
        "\n",
        "with open(file_path_new, 'r', encoding='utf-8') as file:  # Ensure proper encoding\n",
        "    for line in file:\n",
        "        line_labels = line.strip().split()  # Split each line into labels\n",
        "        pred_labels.append(line_labels)  # Append the list of labels to the main list\n",
        "\n",
        "# Show the first 5 entries of pred_labels to verify the operation\n",
        "#pred_labels[:5]\n"
      ],
      "metadata": {
        "id": "3xOjBVna4V_R",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(gold_labels)"
      ],
      "metadata": {
        "id": "yUhOv2msQuKJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pred_labels)"
      ],
      "metadata": {
        "id": "Omng0jycQ0Hu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_results = {'correct': 0, 'incorrect': 0, 'partial': 0,\n",
        "                   'missed': 0, 'spurious': 0, 'possible': 0, 'actual': 0, 'precision': 0, 'recall': 0}\n",
        "\n",
        "# overall results\n",
        "results = {'strict': deepcopy(metrics_results),\n",
        "           'ent_type': deepcopy(metrics_results),\n",
        "           'partial':deepcopy(metrics_results),\n",
        "           'exact':deepcopy(metrics_results)\n",
        "          }\n",
        "\n",
        "\n",
        "# results aggregated by entity type\n",
        "evaluation_agg_entities_type = {e: deepcopy(results) for e in ['Social_Impacts', 'Clinical_Impacts']}\n",
        "\n",
        "for true_ents, pred_ents in zip(gold_labels, pred_labels):\n",
        "\n",
        "    # compute results for one message\n",
        "    tmp_results, tmp_agg_results = compute_metrics(\n",
        "        collect_named_entities(true_ents), collect_named_entities(pred_ents),  ['Social_Impacts', 'Clinical_Impacts']\n",
        "    )\n",
        "\n",
        "    #print(tmp_results)\n",
        "\n",
        "    # aggregate overall results\n",
        "    for eval_schema in results.keys():\n",
        "        for metric in metrics_results.keys():\n",
        "            results[eval_schema][metric] += tmp_results[eval_schema][metric]\n",
        "\n",
        "    # Calculate global precision and recall\n",
        "\n",
        "    results = compute_precision_recall_wrapper(results)\n",
        "\n",
        "\n",
        "    # aggregate results by entity type\n",
        "\n",
        "    for e_type in ['Social_Impacts', 'Clinical_Impacts']:\n",
        "\n",
        "        for eval_schema in tmp_agg_results[e_type]:\n",
        "\n",
        "            for metric in tmp_agg_results[e_type][eval_schema]:\n",
        "\n",
        "                evaluation_agg_entities_type[e_type][eval_schema][metric] += tmp_agg_results[e_type][eval_schema][metric]\n",
        "\n",
        "        # Calculate precision recall at the individual entity level\n",
        "\n",
        "        evaluation_agg_entities_type[e_type] = compute_precision_recall_wrapper(evaluation_agg_entities_type[e_type])\n",
        "\n"
      ],
      "metadata": {
        "id": "Y1z6s9_zZUEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I usually use \"ent_type\" or \"strict\""
      ],
      "metadata": {
        "id": "XWqWvKDuyTBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision = results['ent_type']['precision']\n",
        "recall = results['ent_type']['recall']\n",
        "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0"
      ],
      "metadata": {
        "id": "lwxeQdsHbiz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['strict']['spurious']"
      ],
      "metadata": {
        "id": "cl0oAS51uz2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['strict']"
      ],
      "metadata": {
        "id": "QXWXy4Msu3TM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision"
      ],
      "metadata": {
        "id": "UiK-DUkvyoVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall"
      ],
      "metadata": {
        "id": "qTrEZCHXyqYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score"
      ],
      "metadata": {
        "id": "XDCK8qBXyq8c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}